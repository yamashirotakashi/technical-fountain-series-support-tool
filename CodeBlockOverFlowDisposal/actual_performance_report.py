#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Actual Performance Report - ä¿®æ­£ç‰ˆæ¤œå‡ºå™¨ã®å®Ÿéš›ã®æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆ
"""

def generate_actual_performance_report():
    """ä¿®æ­£ç‰ˆæ¤œå‡ºå™¨ã®å®Ÿéš›ã®æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆï¼ˆãƒ†ã‚¹ãƒˆçµæœãƒ™ãƒ¼ã‚¹ï¼‰"""
    
    # Ground Truth (å…ƒã®Ground Truth)
    original_ground_truth = {
        'sample.pdf': [48],
        'sample2.pdf': [128, 129],
        'sample3.pdf': [13, 35, 36, 39, 42, 44, 45, 47, 49, 62, 70, 78, 80, 106, 115, 122, 124],
        'sample4.pdf': [27, 30, 38, 73, 75, 76],
        'sample5.pdf': [128, 129]
    }
    
    # Fixed Maximum OCR Detector ã®å®Ÿéš›ã®çµæœï¼ˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‹ã‚‰å–å¾—ï¼‰
    actual_fixed_detector_results = {
        'sample.pdf': [],  # 0ãƒšãƒ¼ã‚¸æ¤œå‡º
        'sample2.pdf': [],  # 0ãƒšãƒ¼ã‚¸æ¤œå‡º
        'sample3.pdf': [13, 35, 36, 39, 42, 44, 45, 47, 49, 62, 70, 75, 78, 79, 115, 121, 122, 124, 125],  # 19ãƒšãƒ¼ã‚¸æ¤œå‡º
        'sample4.pdf': [27, 30, 73, 76],  # 4ãƒšãƒ¼ã‚¸æ¤œå‡º
        'sample5.pdf': []   # 0ãƒšãƒ¼ã‚¸æ¤œå‡º
    }
    
    print("Fixed Maximum OCR Detector - å®Ÿéš›ã®æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆ")
    print("=" * 90)
    
    def calculate_metrics(results_dict, ground_truth_dict):
        """æ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
        total_expected = sum(len(pages) for pages in ground_truth_dict.values())
        total_detected = sum(len(pages) for pages in results_dict.values())
        
        true_positives = 0
        false_positives = 0
        false_negatives = 0
        
        for pdf_file, expected in ground_truth_dict.items():
            detected = results_dict.get(pdf_file, [])
            
            tp = len([p for p in detected if p in expected])
            fp = len([p for p in detected if p not in expected])
            fn = len([p for p in expected if p not in detected])
            
            true_positives += tp
            false_positives += fp
            false_negatives += fn
        
        recall = true_positives / total_expected if total_expected > 0 else 0
        precision = true_positives / total_detected if total_detected > 0 else 1.0
        
        return {
            'total_expected': total_expected,
            'total_detected': total_detected,
            'true_positives': true_positives,
            'false_positives': false_positives,
            'false_negatives': false_negatives,
            'recall': recall,
            'precision': precision
        }
    
    # æ–°æ¤œå‡ºå™¨ã®æ€§èƒ½
    new_metrics = calculate_metrics(actual_fixed_detector_results, original_ground_truth)
    
    print(f"\nğŸ“Š ä¿®æ­£ç‰ˆæ¤œå‡ºå™¨ã®å®Ÿéš›ã®æ€§èƒ½:")
    print("-" * 70)
    print(f"ç·æ¤œå‡ºãƒšãƒ¼ã‚¸æ•°: {new_metrics['total_detected']}")
    print(f"æœŸå¾…ãƒšãƒ¼ã‚¸æ•°: {new_metrics['total_expected']}")
    print(f"æ­£æ¤œå‡º: {new_metrics['true_positives']}")
    print(f"èª¤æ¤œå‡º: {new_metrics['false_positives']}")
    print(f"è¦‹é€ƒã—: {new_metrics['false_negatives']}")
    print(f"Recall: {new_metrics['recall']:.1%}")
    print(f"Precision: {new_metrics['precision']:.1%}")
    
    print(f"\nğŸ¯ ç›®æ¨™é”æˆçŠ¶æ³:")
    print("-" * 50)
    print(f"Phase 1ç›®æ¨™ï¼ˆ78%ï¼‰: {'âœ… é”æˆ' if new_metrics['recall'] >= 0.78 else 'âŒ æœªé”æˆ'}")
    print(f"æœ€çµ‚ç›®æ¨™ï¼ˆ85%ï¼‰  : {'âœ… é”æˆ' if new_metrics['recall'] >= 0.85 else 'âŒ æœªé”æˆ'}")
    print(f"ç¾åœ¨ã®Recall    : {new_metrics['recall']:.1%}")
    
    if new_metrics['recall'] >= 0.85:
        print(f"ğŸ‰ æœ€çµ‚ç›®æ¨™é”æˆï¼Phase 2ã¯ä¸è¦ã§ã™ã€‚")
    elif new_metrics['recall'] >= 0.78:
        needed_additional = int(28 * (0.85 - new_metrics['recall']))
        print(f"âš ï¸  Phase 1ç›®æ¨™é”æˆã€æ®‹ã‚Š{needed_additional}ãƒšãƒ¼ã‚¸ã§Phase 2ä¸è¦")
    else:
        needed_additional = int(28 * (0.78 - new_metrics['recall']))
        print(f"âŒ Phase 1ç›®æ¨™æœªé”æˆã€{needed_additional}ãƒšãƒ¼ã‚¸ã®è¿½åŠ æ”¹å–„ãŒå¿…è¦")
    
    print(f"\nğŸ“ˆ è©³ç´°åˆ†æ:")
    print("-" * 50)
    
    # PDFåˆ¥è©³ç´°
    for pdf_file, expected in original_ground_truth.items():
        detected = actual_fixed_detector_results.get(pdf_file, [])
        
        tp = len([p for p in detected if p in expected])
        fp = len([p for p in detected if p not in expected])
        fn = len([p for p in expected if p not in detected])
        
        recall_file = tp / len(expected) if expected else 1.0
        precision_file = tp / len(detected) if detected else 1.0
        
        print(f"{pdf_file}:")
        print(f"  æœŸå¾…: {len(expected)}ãƒšãƒ¼ã‚¸, æ¤œå‡º: {len(detected)}ãƒšãƒ¼ã‚¸")
        print(f"  æ­£æ¤œå‡º: {tp}, èª¤æ¤œå‡º: {fp}, è¦‹é€ƒã—: {fn}")
        print(f"  Recall: {recall_file:.1%}, Precision: {precision_file:.1%}")
        
        if fn > 0:
            missed_pages = [p for p in expected if p not in detected]
            print(f"  è¦‹é€ƒã—ãƒšãƒ¼ã‚¸: {missed_pages}")
        
        if fp > 0:
            false_pages = [p for p in detected if p not in expected]
            print(f"  èª¤æ¤œçŸ¥ãƒšãƒ¼ã‚¸: {false_pages}")
    
    print(f"\nğŸ” å•é¡Œåˆ†æ:")
    print("-" * 50)
    print(f"ä¸»è¦ãªå•é¡Œ:")
    print(f"1. sample.pdf, sample2.pdf, sample5.pdf ã§æ¤œå‡ºå¤±æ•—")
    print(f"2. sample3.pdf ã§èª¤æ¤œçŸ¥ãŒ4ãƒšãƒ¼ã‚¸ï¼ˆ75, 79, 121, 125ï¼‰")
    print(f"3. sample3.pdf ã§è¦‹é€ƒã—ãŒ2ãƒšãƒ¼ã‚¸ï¼ˆ80, 106ï¼‰")
    print(f"4. sample4.pdf ã§è¦‹é€ƒã—ãŒ2ãƒšãƒ¼ã‚¸ï¼ˆ38, 75ï¼‰")
    
    print(f"\nğŸ’¡ æ”¹å–„ææ¡ˆ:")
    print("-" * 50)
    print(f"1. sample.pdf(p48), sample2/5.pdf(p128,129)ã®ç‰¹æ®Šãƒ‘ã‚¿ãƒ¼ãƒ³èª¿æŸ»")
    print(f"2. èª¤æ¤œçŸ¥4ãƒšãƒ¼ã‚¸ã®åŸå› åˆ†æã¨è¿½åŠ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°")
    print(f"3. è¦‹é€ƒã—4ãƒšãƒ¼ã‚¸ã®æ¤œå‡ºæ¡ä»¶ã®ç·©å’Œ")
    print(f"4. é©å¿œçš„é–¾å€¤ã®èª¿æ•´")
    
    print(f"\nğŸ’¡ çµè«–:")
    print("=" * 90)
    print(f"âœ… èª¤æ¤œçŸ¥å•é¡Œï¼ˆ540â†’4ãƒšãƒ¼ã‚¸ï¼‰ã¯è§£æ±ºæ¸ˆã¿")
    print(f"âœ… Precisionå¤§å¹…æ”¹å–„ï¼ˆ4.9%â†’{new_metrics['precision']:.1%}ï¼‰")
    print(f"âŒ Recall ã¯ {new_metrics['recall']:.1%} ã§ç›®æ¨™78%æœªé”æˆ")
    print(f"âŒ Phase 1ç›®æ¨™é”æˆã«ã¯è¿½åŠ æ”¹å–„ãŒå¿…è¦")
    print("=" * 90)

if __name__ == "__main__":
    generate_actual_performance_report()